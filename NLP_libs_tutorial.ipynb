{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import RAKE\n",
    "import textacy\n",
    "\n",
    "# nltk\n",
    "# rake\n",
    "# TextBlob\n",
    "# SpaCy\n",
    "# Textacy\n",
    "# Gensim\n",
    "# CoreNLP\n",
    "\n",
    "\n",
    "# Pandas indexing note:\n",
    "    # loc - query by labels\n",
    "    # iloc - query by indices\n",
    "    # ix - query by labels, falls back to indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(text1)\n",
    "print(text1.concordance(\"monstrous\"))\n",
    "print(text1.similar(\"monstrous\"))\n",
    "print(text2.common_contexts([\"monstrous\", \"very\"]))\n",
    "\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
    "\n",
    "# text3.generate()\n",
    "\n",
    "print(sorted(set(text3)))\n",
    "print(FreqDist(text1))\n",
    "print(list(bigrams(['more', 'is', 'said', 'than', 'done'])))\n",
    "\n",
    "fdist = FreqDist(len(w) for w in text1)\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction with Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minimal generating sets', 8.666666666666666),\n",
       " ('linear diophantine equations', 8.5),\n",
       " ('minimal supporting set', 7.666666666666666),\n",
       " ('minimal set', 4.666666666666666),\n",
       " ('linear constraints', 4.5),\n",
       " ('upper bounds', 4.0),\n",
       " ('natural numbers', 4.0),\n",
       " ('nonstrict inequations', 4.0),\n",
       " ('strict inequations', 4.0),\n",
       " ('mixed types', 3.666666666666667),\n",
       " ('considered types', 3.166666666666667),\n",
       " ('set', 2.0),\n",
       " ('types', 1.6666666666666667),\n",
       " ('considered', 1.5),\n",
       " ('constructing', 1.0),\n",
       " ('solutions', 1.0),\n",
       " ('solving', 1.0),\n",
       " ('system', 1.0),\n",
       " ('compatibility', 1.0),\n",
       " ('systems', 1.0),\n",
       " ('criteria', 1.0),\n",
       " ('construction', 1.0),\n",
       " ('algorithms', 1.0),\n",
       " ('components', 1.0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rake = RAKE.Rake('SmartStoplist.txt')\n",
    "Rake.run(\"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility \" \\\n",
    "       \"of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. \" \\\n",
    "       \"Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating\"\\\n",
    "       \" sets of solutions for all types of systems are given. These criteria and the corresponding algorithms \" \\\n",
    "       \"for constructing a minimal supporting set of solutions can be used in solving all the considered types of \" \\\n",
    "       \"systems and systems of mixed types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy analysis with Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'survey_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-94d4605943c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurvey_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopinion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopinion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'survey_data' is not defined"
     ]
    }
   ],
   "source": [
    "text = survey_data.iloc[121,3]\n",
    "print(text)\n",
    "opinion = TextBlob(text)\n",
    "opinion.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', u'DT'), ('titular', u'JJ'), ('threat', u'NN'), ('of', u'IN'), ('The', u'DT'), ('Blob', u'NNP'), ('has', u'VBZ'), ('always', u'RB'), ('struck', u'VBN'), ('me', u'PRP'), ('as', u'IN'), ('the', u'DT'), ('ultimate', u'JJ'), ('movie', u'NN'), ('monster', u'NN'), ('an', u'DT'), ('insatiably', u'RB'), ('hungry', u'JJ'), ('amoeba-like', u'JJ'), ('mass', u'NN'), ('able', u'JJ'), ('to', u'TO'), ('penetrate', u'VB'), ('virtually', u'RB'), ('any', u'DT'), ('safeguard', u'NN'), ('capable', u'JJ'), ('of', u'IN'), ('as', u'IN'), ('a', u'DT'), ('doomed', u'JJ'), ('doctor', u'NN'), ('chillingly', u'RB'), ('describes', u'VBZ'), ('it', u'PRP'), ('assimilating', u'VBG'), ('flesh', u'NN'), ('on', u'IN'), ('contact', u'NN'), ('Snide', u'JJ'), ('comparisons', u'NNS'), ('to', u'TO'), ('gelatin', u'VB'), ('be', u'VB'), ('damned', u'VBN'), ('it', u'PRP'), (\"'s\", u'VBZ'), ('a', u'DT'), ('concept', u'NN'), ('with', u'IN'), ('the', u'DT'), ('most', u'RBS'), ('devastating', u'JJ'), ('of', u'IN'), ('potential', u'JJ'), ('consequences', u'NNS'), ('not', u'RB'), ('unlike', u'IN'), ('the', u'DT'), ('grey', u'NN'), ('goo', u'NN'), ('scenario', u'NN'), ('proposed', u'VBN'), ('by', u'IN'), ('technological', u'JJ'), ('theorists', u'NNS'), ('fearful', u'NN'), ('of', u'IN'), ('artificial', u'JJ'), ('intelligence', u'NN'), ('run', u'NN'), ('rampant', u'NN')]\n",
      "[u'titular threat', 'blob', u'ultimate movie monster', u'amoeba-like mass', 'snide', u'potential consequences', u'grey goo scenario', u'technological theorists fearful', u'artificial intelligence run rampant']\n",
      "0.06\n",
      "-0.341666666667\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "print(blob.tags)\n",
    "\n",
    "print(blob.noun_phrases)\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)\n",
    "\n",
    "# blob.translate(to=\"es\")  # 'La amenaza titular de The Blob...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cats', 'dogs', 'octopodes'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import WordList\n",
    "\n",
    "animals = TextBlob(\"cat dog octopus\")\n",
    "animals.words\n",
    "\n",
    "WordList(['cat', 'dog', 'octopus'])\n",
    "animals.words.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'fallibility', 1.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())\n",
    "\n",
    "from textblob import Word\n",
    "w = Word('falibility')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving deeper with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Hello, world. Natural Language Processing in 10 lines of code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tokens and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Natural Language Processing in 10 lines of code.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello - INTJ\n",
      ", - PUNCT\n",
      "world - NOUN\n",
      ". - PUNCT\n",
      "Natural - PROPN\n",
      "Language - PROPN\n",
      "Processing - PROPN\n",
      "in - ADP\n",
      "10 - NUM\n",
      "lines - NOUN\n",
      "of - ADP\n",
      "code - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello --> []\n",
      ", --> [,, Hello]\n",
      "world --> [world, Hello]\n",
      ". --> [., Hello]\n",
      "Natural --> [Natural, Language, Language, Processing]\n",
      "Language --> [Language, Processing]\n",
      "Processing --> []\n",
      "in --> [in, Processing]\n",
      "10 --> [10, lines, lines, in, in, Processing]\n",
      "lines --> [lines, in, in, Processing]\n",
      "of --> [of, lines, lines, in, in, Processing]\n",
      "code --> [code, of, of, lines, lines, in, in, Processing]\n",
      ". --> [., Processing]\n",
      "\n",
      ",-punct-> Hello-ROOT\n",
      "world-npadvmod-> Hello-ROOT\n",
      ".-punct-> Hello-ROOT\n",
      "Natural-compound-> Language-compound-> Language-compound-> Processing-ROOT\n",
      "Language-compound-> Processing-ROOT\n",
      "\n",
      "in-prep-> Processing-ROOT\n",
      "10-nummod-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "code-pobj-> of-prep-> of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      ".-punct-> Processing-ROOT\n"
     ]
    }
   ],
   "source": [
    "# Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris - GPE\n",
      "Jack - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "doc_2 = nlp(u\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Paris, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I, ',', -3.791565179824829)\n",
      "(went, ',', -8.091073989868164)\n",
      "(to, ',', -3.8560216426849365)\n",
      "(Paris, ',', -11.824886322021484)\n",
      "(where, ',', -7.146170139312744)\n",
      "(I, ',', -3.791565179824829)\n",
      "(met, ',', -9.557488441467285)\n",
      "(my, ',', -5.491642951965332)\n",
      "(old, ',', -7.845602989196777)\n",
      "(friend, ',', -8.210515975952148)\n",
      "(Jack, ',', -10.852154731750488)\n",
      "(from, ',', -6.010132312774658)\n",
      "(uni, ',', -11.876206398010254)\n",
      "(., ',', -3.0678977966308594)\n"
     ]
    }
   ],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding / Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77809414836\n",
      "0.038474555379\n",
      "()\n",
      "0.667620716334\n",
      "0.332848635361\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving deeper with Textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiently stream documents from disk and into a processed corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus(1241 docs; 857315 tokens)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw = textacy.corpora.CapitolWords()\n",
    "docs = cw.records(speaker_name={'Hillary Clinton', 'Barack Obama'})\n",
    "content_stream, metadata_stream = textacy.fileio.split_record_fields(\n",
    "    docs, 'text')\n",
    "corpus = textacy.Corpus(u'en', texts=content_stream, metadatas=metadata_stream)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent corpus as a document-term matrix, with flexible weighting and filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1241x11338 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 211383 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix, id2term = textacy.vsm.doc_term_matrix(\n",
    "    (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "     for doc in corpus),\n",
    "    weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)\n",
    "print(repr(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and interpret a topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 10)\n",
      "('topic', 0, ':', u\"new   's   child   people   need   year   today   work   york   state\")\n",
      "('topic', 1, ':', u'rescind   quorum   order   consent   unanimous   ask   president   mr.   madam   objection')\n",
      "('topic', 2, ':', u'dispense   reading   consent   unanimous   amendment   ask   president   mr.   madam   presiding')\n",
      "('topic', 3, ':', u'virginia   west virginia   west   senator   yield   question   thank   objection   inquiry   massachusetts')\n",
      "('topic', 4, ':', u'senators   desiring   chamber   vote   airway   railway   expedited   package   upgrade   assurance')\n",
      "('topic', 5, ':', u'amendment   pending   aside   set   unanimous   ask   consent   mr.   president   desk')\n",
      "('topic', 6, ':', u'health   care   mental   patient   child   quality   medical   information   americans   hospital')\n",
      "('topic', 7, ':', u'iraq   iraqi   war   troop   iraqis   american   u.s.   afghanistan   military   policy')\n",
      "('topic', 8, ':', u'senate   thursday   wednesday   session   unanimous   consent   authorize   p.m.   committee   september')\n",
      "('topic', 9, ':', u'medicare   drug   senior   medicaid   prescription   benefit   plan   cut   fda   cost')\n"
     ]
    }
   ],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "print(doc_topic_matrix.shape)\n",
    "\n",
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic indexing as well as flexible selection of documents in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n",
      "Doc(2999 tokens; \"In the Federalist Papers, we often hear the ref...\")\n"
     ]
    }
   ],
   "source": [
    "obama_docs = list(corpus.get(lambda doc: doc.metadata['speaker_name'] == 'Barack Obama'))\n",
    "print(len(obama_docs))\n",
    "doc = corpus[-1]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess plain text, or highlight particular terms in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g on this tiny piece of Senate and  America n history. Some 10 years ago, I ask\n",
      "o do the hard work in New York and  America , who get up every day and do the v\n",
      "say: You know, you never can count  America  out. Whenever the chips are down, \n",
      " what we know will give our fellow  America ns a better shot at the kind of fut\n",
      "aith in this body and in my fellow  America ns. I remain an optimist, that Amer\n",
      "ricans. I remain an optimist, that  America 's best days are still ahead of us.\n"
     ]
    }
   ],
   "source": [
    "textacy.preprocess_text(doc.text, lowercase=True, no_punct=True)[:70]\n",
    "textacy.text_utils.keyword_in_context(doc.text, 'America', window_width=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract various elements of interest from parsed documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Federalist Papers, Senate's, 's role, violent passions, pernicious resolutions, everlasting credit, common ground, 8 years, tiny piece, American history, 10 years, years ago, New York, fellow New, New Yorkers]\n",
      "--------------------\n",
      "[fellow New Yorkers, World Trade Center, Senator from New, World Trade Center, Senator from New, lot of fun, fellow New Yorkers, lot of fun]\n",
      "--------------------\n",
      "[Senate, Senate, American, New York, New Yorkers, Senate, Barbara Mikulski, Senate, Pennsylvania Avenue, Senate]\n",
      "--------------------\n",
      "<DET>? <NUM>* (<ADJ> <PUNCT>? <CONJ>?)* (<NOUN>|<PROPN> <PART>?)+\n",
      "--------------------\n",
      "[the Federalist Papers, the reference, the Senate's role, the consequences, sudden and violent passions, intemperate and pernicious resolutions, the everlasting credit, wisdom, our Founders, an effort]\n",
      "--------------------\n",
      "[(I, was, on the other end of Pennsylvania Avenue), (I, was, , a very new Senator, and my city and my State had been devastated), (I, am, grateful to have had Senator Schumer as my partner and my ally), (I, am, very excited about what can happen in the next 4 years), (I, been, a New Yorker, but I know I always will be one)]\n",
      "--------------------\n",
      "[(u'new', 0.009979242951283706), (u'senator', 0.007625352003459275), (u'york', 0.006706167538922378), (u'day', 0.0059250450259880905), (u'senate', 0.005822285168371922), (u'people', 0.005352999513186566), (u'state', 0.0051647606925409525), (u'joshua', 0.004841752399636475), (u'year', 0.004376843623803211), (u'way', 0.00432178729004174)]\n"
     ]
    }
   ],
   "source": [
    "print(list(textacy.extract.ngrams(doc, 2, filter_stops=True, filter_punct=True, filter_nums=False))[:15])\n",
    "print('-'*20)\n",
    "print(list(textacy.extract.ngrams(doc, 3, filter_stops=True, filter_punct=True, min_freq=2)))\n",
    "print('-'*20)\n",
    "print(list(textacy.extract.named_entities(doc, drop_determiners=True, exclude_types=u'numeric'))[:10])\n",
    "print('-'*20)\n",
    "\n",
    "pattern = textacy.constants.POS_REGEX_PATTERNS['en']['NP']\n",
    "print(pattern)\n",
    "print('-'*20)\n",
    "\n",
    "print(list(textacy.extract.pos_regex_matches(doc, pattern))[:10])\n",
    "print('-'*20)\n",
    "print(list(textacy.extract.semistructured_statements(doc, 'I', cue='be')))\n",
    "print('-'*20)\n",
    "print(textacy.keyterms.textrank(doc, n_keyterms=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute common statistical attributes of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'automated_readability_index': 12.674475357710648,\n",
       " u'coleman_liau_index': 9.893869608108112,\n",
       " u'flesch_kincaid_grade_level': 10.754593958664547,\n",
       " u'flesch_readability_ease': 62.77017551669317,\n",
       " u'gunning_fog_index': 13.593411764705884,\n",
       " u'n_chars': 11498,\n",
       " u'n_polysyllable_words': 222,\n",
       " u'n_sents': 100,\n",
       " u'n_syllables': 3525,\n",
       " u'n_unique_words': 1107,\n",
       " u'n_words': 2516,\n",
       " u'smog_index': 11.640900244366641}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.text_stats.readability_stats(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count terms individually, and represent documents as a bag-of-terms with flexible weighting and inclusion criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'term_len' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-37e08aa69795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'America'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bag_of_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianjohnson/.virtualenvs/unilever/lib/python2.7/site-packages/textacy/doc.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, term)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mterm_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# we haven't counted terms of this length; let's do that now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mterm_len\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counted_ngrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterm_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 self._counts += Counter(\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'term_len' referenced before assignment"
     ]
    }
   ],
   "source": [
    "print(doc.count('America'))\n",
    "\n",
    "bot = doc.to_bag_of_terms(ngrams={2, 3}, as_strings=True)\n",
    "sorted(bot.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-01 15:15:31,039 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],\n",
    "    [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],\n",
    "    [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],\n",
    "    [(0, 1.0), (4, 2.0), (7, 1.0)],\n",
    "    [(3, 1.0), (5, 1.0), (6, 1.0)],\n",
    "    [(9, 1.0)],\n",
    "    [(9, 1.0), (10, 1.0)],\n",
    "    [(9, 1.0), (10, 1.0), (11, 1.0)],\n",
    "    [(8, 1.0), (10, 1.0), (11, 1.0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-01 15:15:32,575 : INFO : collecting document frequencies\n",
      "2017-03-01 15:15:32,576 : INFO : PROGRESS: processing document #0\n",
      "2017-03-01 15:15:32,578 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n",
      "2017-03-01 15:15:32,580 : INFO : creating sparse index\n",
      "2017-03-01 15:15:32,582 : INFO : creating sparse matrix from corpus\n",
      "2017-03-01 15:15:32,583 : INFO : PROGRESS: at document #0\n",
      "2017-03-01 15:15:32,585 : INFO : created <9x12 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 28 stored elements in Compressed Sparse Row format>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8075244024440723), (4, 0.5898341626740045)]\n",
      "[(0, 0.4662244), (1, 0.19139354), (2, 0.24600551), (3, 0.82094586), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "vec = [(0, 1), (4, 1)]\n",
    "print(tfidf[vec])\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)\n",
    "sims = index[tfidf[vec]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora and Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 10:45:45,823 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-03-02 10:45:45,847 : INFO : built Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...) from 9 documents (total 29 corpus positions)\n",
      "2017-03-02 10:45:45,858 : INFO : saving Dictionary object under /tmp/deerwester.dict, separately None\n",
      "2017-03-02 10:45:45,893 : INFO : saved /tmp/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:10:20,153 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "2017-03-02 11:10:20,156 : INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "2017-03-02 11:10:20,157 : INFO : PROGRESS: saving document #0\n",
      "2017-03-02 11:10:20,158 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2017-03-02 11:10:20,160 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (6, 1), (7, 1), (8, 1)], [(2, 1), (6, 2), (8, 1)], [(3, 1), (4, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(5, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Streaming â€“ One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            print(line)\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x11bef7250>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "\n",
      "[]\n",
      "This is \n",
      "\n",
      "[]\n",
      "A human computer\n",
      "\n",
      "[(1, 1), (2, 1)]\n",
      "interaction\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:13:31,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-03-02 11:13:31,153 : INFO : built Dictionary(7 unique tokens: [u'a', u'interaction', u'this', u'is', u'computer']...) from 4 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(0 unique tokens: [])\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:13:39,806 : INFO : storing corpus in Matrix Market format to /tmp/corpus.mm\n",
      "2017-03-02 11:13:39,807 : INFO : saving sparse matrix to /tmp/corpus.mm\n",
      "2017-03-02 11:13:39,809 : INFO : PROGRESS: saving document #0\n",
      "2017-03-02 11:13:39,810 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2017-03-02 11:13:39,812 : INFO : saving MmCorpus index to /tmp/corpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "# create a toy corpus of 2 documents, as a plain Python list\n",
    "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
    "\n",
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:13:59,218 : INFO : converting corpus to SVMlight format: /tmp/corpus.svmlight\n",
      "2017-03-02 11:13:59,221 : INFO : saving SvmLightCorpus index to /tmp/corpus.svmlight.index\n",
      "2017-03-02 11:13:59,223 : INFO : no word id mapping provided; initializing from corpus\n",
      "2017-03-02 11:13:59,225 : INFO : storing corpus in Blei's LDA-C format into /tmp/corpus.lda-c\n",
      "2017-03-02 11:13:59,227 : INFO : saving vocabulary of 2 words to /tmp/corpus.lda-c.vocab\n",
      "2017-03-02 11:13:59,229 : INFO : saving BleiCorpus index to /tmp/corpus.lda-c.index\n",
      "2017-03-02 11:13:59,231 : INFO : no word id mapping provided; initializing from corpus\n",
      "2017-03-02 11:13:59,232 : INFO : storing corpus in List-Of-Words format into /tmp/corpus.low\n",
      "2017-03-02 11:13:59,235 : WARNING : List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value\n",
      "2017-03-02 11:13:59,236 : INFO : saving LowCorpus index to /tmp/corpus.low.index\n"
     ]
    }
   ],
   "source": [
    "corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)\n",
    "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)\n",
    "corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:14:24,990 : INFO : loaded corpus index from /tmp/corpus.mm.index\n",
      "2017-03-02 11:14:24,991 : INFO : initializing corpus reader from /tmp/corpus.mm\n",
      "2017-03-02 11:14:24,993 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus('/tmp/corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 0.5)], []]\n"
     ]
    }
   ],
   "source": [
    "# one way of printing a corpus: load it entirely into memory\n",
    "print(list(corpus))  # calling list() will convert any sequence to a plain Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.5)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# another way of doing it: print one document at a time, making use of the streaming interface\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:14:30,952 : INFO : no word id mapping provided; initializing from corpus\n",
      "2017-03-02 11:14:30,954 : INFO : storing corpus in Blei's LDA-C format into /tmp/corpus.lda-c\n",
      "2017-03-02 11:14:30,956 : INFO : saving vocabulary of 2 words to /tmp/corpus.lda-c.vocab\n",
      "2017-03-02 11:14:30,957 : INFO : saving BleiCorpus index to /tmp/corpus.lda-c.index\n"
     ]
    }
   ],
   "source": [
    "corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with NumPy and SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.],\n",
       "       [ 4.,  2.],\n",
       "       [ 2.,  2.],\n",
       "       [ 3.,  9.],\n",
       "       [ 4.,  7.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "numpy_matrix = np.random.randint(10, size=[5,2])  # random matrix as an example\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "gensim.matutils.corpus2dense(corpus, num_terms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<0x2 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy_sparse_matrix = scipy.sparse.random(5,2)  # random sparse matrix as example\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "gensim.matutils.corpus2csc(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:16:34,956 : INFO : loading Dictionary object from /tmp/deerwester.dict\n",
      "2017-03-02 11:16:34,958 : INFO : loaded /tmp/deerwester.dict\n",
      "2017-03-02 11:16:34,960 : INFO : loaded corpus index from /tmp/deerwester.mm.index\n",
      "2017-03-02 11:16:34,961 : INFO : initializing corpus reader from /tmp/deerwester.mm\n",
      "2017-03-02 11:16:34,962 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "if (os.path.exists(\"/tmp/deerwester.dict\")):\n",
    "   dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "   corpus = corpora.MmCorpus('/tmp/deerwester.mm')\n",
    "   print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "   print(\"Please run first tutorial to generate data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:16:50,920 : INFO : collecting document frequencies\n",
      "2017-03-02 11:16:50,923 : INFO : PROGRESS: processing document #0\n",
      "2017-03-02 11:16:50,925 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:17:02,585 : INFO : using serial LSI version on this node\n",
      "2017-03-02 11:17:02,587 : INFO : updating model with new documents\n",
      "2017-03-02 11:17:02,590 : INFO : preparing a new chunk of documents\n",
      "2017-03-02 11:17:02,592 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-03-02 11:17:02,593 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2017-03-02 11:17:02,596 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2017-03-02 11:17:02,607 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-03-02 11:17:02,612 : INFO : computing the final decomposition\n",
      "2017-03-02 11:17:02,615 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2017-03-02 11:17:02,618 : INFO : processed documents up to #9\n",
      "2017-03-02 11:17:02,620 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2017-03-02 11:17:02,621 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:17:09,713 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2017-03-02 11:17:09,715 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  u'-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066007833960903969), (1, -0.52007033063618402)]\n",
      "[(0, 0.19667592859142538), (1, -0.76095631677000464)]\n",
      "[(0, 0.089926399724465617), (1, -0.72418606267525065)]\n",
      "[(0, 0.075858476521782819), (1, -0.63205515860034223)]\n",
      "[(0, 0.10150299184980148), (1, -0.57373084830029586)]\n",
      "[(0, 0.70321089393783098), (1, 0.16115180214025893)]\n",
      "[(0, 0.87747876731198315), (1, 0.16758906864659542)]\n",
      "[(0, 0.90986246868185794), (1, 0.14086553628719153)]\n",
      "[(0, 0.61658253505692828), (1, -0.053929075663892927)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:17:13,376 : INFO : saving Projection object under /tmp/model.lsi.projection, separately None\n",
      "2017-03-02 11:17:13,382 : INFO : saved /tmp/model.lsi.projection\n",
      "2017-03-02 11:17:13,383 : INFO : saving LsiModel object under /tmp/model.lsi, separately None\n",
      "2017-03-02 11:17:13,385 : INFO : not storing attribute projection\n",
      "2017-03-02 11:17:13,386 : INFO : not storing attribute dispatcher\n",
      "2017-03-02 11:17:13,402 : INFO : saved /tmp/model.lsi\n",
      "2017-03-02 11:17:13,403 : INFO : loading LsiModel object from /tmp/model.lsi\n",
      "2017-03-02 11:17:13,404 : INFO : loading id2word recursively from /tmp/model.lsi.id2word.* with mmap=None\n",
      "2017-03-02 11:17:13,405 : INFO : setting ignored attribute projection to None\n",
      "2017-03-02 11:17:13,406 : INFO : setting ignored attribute dispatcher to None\n",
      "2017-03-02 11:17:13,407 : INFO : loaded /tmp/model.lsi\n",
      "2017-03-02 11:17:13,409 : INFO : loading LsiModel object from /tmp/model.lsi.projection\n",
      "2017-03-02 11:17:13,410 : INFO : loaded /tmp/model.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "lsi.save('/tmp/model.lsi') # same for tfidf, lda, ...\n",
    "lsi = models.LsiModel.load('/tmp/model.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:17:23,264 : INFO : collecting document frequencies\n",
      "2017-03-02 11:17:23,266 : INFO : PROGRESS: processing document #0\n",
      "2017-03-02 11:17:23,268 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "model = models.TfidfModel(corpus, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:17:40,638 : INFO : using serial LSI version on this node\n",
      "2017-03-02 11:17:40,639 : INFO : updating model with new documents\n",
      "2017-03-02 11:17:40,642 : INFO : preparing a new chunk of documents\n",
      "2017-03-02 11:17:40,643 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-03-02 11:17:40,644 : INFO : 1st phase: constructing (12, 400) action matrix\n",
      "2017-03-02 11:17:40,646 : INFO : orthonormalizing (12, 400) action matrix\n",
      "2017-03-02 11:17:40,653 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-03-02 11:17:40,654 : INFO : computing the final decomposition\n",
      "2017-03-02 11:17:40,656 : INFO : keeping 9 factors (discarding 0.000% of energy spectrum)\n",
      "2017-03-02 11:17:40,658 : INFO : processed documents up to #9\n",
      "2017-03-02 11:17:40,660 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2017-03-02 11:17:40,661 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n",
      "2017-03-02 11:17:40,663 : INFO : topic #2(1.191): 0.456*\"response\" + 0.456*\"time\" + -0.352*\"eps\" + -0.340*\"human\" + -0.318*\"interface\" + -0.277*\"system\" + 0.272*\"survey\" + 0.213*\"user\" + -0.183*\"trees\" + 0.114*\"minors\"\n",
      "2017-03-02 11:17:40,664 : INFO : topic #3(1.043): -0.583*\"trees\" + 0.556*\"minors\" + 0.399*\"survey\" + 0.256*\"graph\" + -0.211*\"time\" + -0.211*\"response\" + -0.160*\"user\" + 0.081*\"human\" + 0.038*\"interface\" + 0.035*\"system\"\n",
      "2017-03-02 11:17:40,666 : INFO : topic #4(0.884): -0.611*\"computer\" + 0.425*\"system\" + 0.420*\"eps\" + -0.354*\"interface\" + -0.339*\"human\" + 0.148*\"user\" + 0.058*\"minors\" + -0.047*\"trees\" + 0.034*\"graph\" + -0.027*\"survey\"\n"
     ]
    }
   ],
   "source": [
    "model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'another_tfidf_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-886d039dec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manother_tfidf_corpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now LSI has been trained on tfidf_corpus + another_tfidf_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlsi_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_vec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# convert some new document into the LSI space, without affecting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_documents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tfidf_corpus + another_tfidf_corpus + more_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlsi_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'another_tfidf_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "model.add_documents(another_tfidf_corpus) # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "lsi_vec = model[tfidf_vec] # convert some new document into the LSI space, without affecting the model\n",
    "\n",
    "model.add_documents(more_documents) # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "lsi_vec = model[tfidf_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:18:15,338 : INFO : no word id mapping provided; initializing from corpus, assuming identity\n",
      "2017-03-02 11:18:15,341 : INFO : constructing (500, 12) random matrix\n",
      "2017-03-02 11:18:15,344 : INFO : using symmetric alpha at 0.01\n",
      "2017-03-02 11:18:15,345 : INFO : using symmetric eta at 0.0833333333333\n",
      "2017-03-02 11:18:15,346 : INFO : using serial LDA version on this node\n",
      "2017-03-02 11:18:15,356 : INFO : running online LDA training, 100 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-03-02 11:18:15,357 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2017-03-02 11:18:15,377 : INFO : -124.661 per-word bound, 33616746376526463495493190672153837568.0 perplexity estimate based on a held-out corpus of 9 documents with 29 words\n",
      "2017-03-02 11:18:15,380 : INFO : PROGRESS: pass 0, at document #9/9\n",
      "2017-03-02 11:18:15,390 : INFO : topic #15 (0.010): 0.083*\"user\" + 0.083*\"survey\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"interface\" + 0.083*\"system\" + 0.083*\"human\" + 0.083*\"response\" + 0.083*\"computer\"\n",
      "2017-03-02 11:18:15,391 : INFO : topic #58 (0.010): 0.083*\"user\" + 0.083*\"survey\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"interface\" + 0.083*\"system\" + 0.083*\"human\" + 0.083*\"response\" + 0.083*\"computer\"\n",
      "2017-03-02 11:18:15,393 : INFO : topic #85 (0.010): 0.083*\"user\" + 0.083*\"survey\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"interface\" + 0.083*\"system\" + 0.083*\"human\" + 0.083*\"response\" + 0.083*\"computer\"\n",
      "2017-03-02 11:18:15,394 : INFO : topic #5 (0.010): 0.083*\"user\" + 0.083*\"survey\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"interface\" + 0.083*\"system\" + 0.083*\"human\" + 0.083*\"response\" + 0.083*\"computer\"\n",
      "2017-03-02 11:18:15,395 : INFO : topic #52 (0.010): 0.083*\"user\" + 0.083*\"survey\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"interface\" + 0.083*\"system\" + 0.083*\"human\" + 0.083*\"response\" + 0.083*\"computer\"\n",
      "2017-03-02 11:18:15,397 : INFO : topic diff=87.298918, rho=1.000000\n",
      "2017-03-02 11:18:15,446 : INFO : (0, u'0.220*eps + 0.211*survey + 0.172*computer + 0.085*time + 0.083*user + 0.071*interface + 0.067*human + 0.030*minors + 0.025*trees + 0.016*graph')\n",
      "2017-03-02 11:18:15,448 : INFO : (1, u'0.366*eps + 0.164*system + 0.145*interface + 0.112*trees + 0.078*minors + 0.044*graph + 0.033*human + 0.032*time + 0.017*user + 0.007*survey')\n",
      "2017-03-02 11:18:15,450 : INFO : (2, u'0.246*graph + 0.192*interface + 0.168*user + 0.119*system + 0.052*response + 0.045*eps + 0.045*minors + 0.044*human + 0.037*time + 0.034*computer')\n",
      "2017-03-02 11:18:15,454 : INFO : (3, u'0.258*minors + 0.138*graph + 0.134*user + 0.128*interface + 0.089*response + 0.080*trees + 0.070*time + 0.034*system + 0.031*eps + 0.027*computer')\n",
      "2017-03-02 11:18:15,456 : INFO : (4, u'0.184*minors + 0.170*interface + 0.166*human + 0.119*time + 0.090*response + 0.075*eps + 0.059*survey + 0.049*graph + 0.038*trees + 0.017*computer')\n",
      "2017-03-02 11:18:15,458 : INFO : (5, u'0.314*graph + 0.306*trees + 0.090*eps + 0.079*interface + 0.048*minors + 0.039*response + 0.039*system + 0.031*computer + 0.026*time + 0.021*survey')\n",
      "2017-03-02 11:18:15,459 : INFO : (6, u'0.341*minors + 0.249*trees + 0.178*survey + 0.064*time + 0.054*eps + 0.031*interface + 0.022*computer + 0.021*user + 0.018*system + 0.012*human')\n",
      "2017-03-02 11:18:15,461 : INFO : (7, u'0.190*user + 0.177*system + 0.160*interface + 0.140*human + 0.121*computer + 0.071*eps + 0.050*response + 0.034*trees + 0.025*time + 0.016*minors')\n",
      "2017-03-02 11:18:15,462 : INFO : (8, u'0.235*survey + 0.152*graph + 0.150*computer + 0.104*time + 0.090*response + 0.066*interface + 0.045*eps + 0.044*user + 0.043*system + 0.037*minors')\n",
      "2017-03-02 11:18:15,464 : INFO : (9, u'0.187*trees + 0.169*human + 0.154*graph + 0.114*response + 0.082*system + 0.073*computer + 0.068*eps + 0.067*minors + 0.035*survey + 0.021*interface')\n",
      "2017-03-02 11:18:15,466 : INFO : (10, u'0.193*interface + 0.192*minors + 0.180*trees + 0.120*graph + 0.102*eps + 0.098*survey + 0.039*time + 0.029*user + 0.016*system + 0.015*response')\n",
      "2017-03-02 11:18:15,468 : INFO : (11, u'0.295*response + 0.194*time + 0.160*eps + 0.155*user + 0.077*system + 0.056*survey + 0.025*human + 0.015*minors + 0.010*trees + 0.009*graph')\n",
      "2017-03-02 11:18:15,469 : INFO : (12, u'0.291*graph + 0.183*computer + 0.119*trees + 0.114*response + 0.100*interface + 0.072*human + 0.032*minors + 0.032*user + 0.026*survey + 0.019*time')\n",
      "2017-03-02 11:18:15,471 : INFO : (13, u'0.238*trees + 0.129*time + 0.100*human + 0.099*system + 0.098*graph + 0.079*eps + 0.070*computer + 0.062*user + 0.041*survey + 0.039*minors')\n",
      "2017-03-02 11:18:15,473 : INFO : (14, u'0.237*time + 0.182*user + 0.177*graph + 0.134*trees + 0.116*system + 0.056*eps + 0.040*minors + 0.023*response + 0.014*interface + 0.012*survey')\n",
      "2017-03-02 11:18:15,474 : INFO : (15, u'0.156*trees + 0.145*computer + 0.134*system + 0.103*time + 0.102*user + 0.092*response + 0.080*minors + 0.061*interface + 0.046*eps + 0.046*survey')\n",
      "2017-03-02 11:18:15,476 : INFO : (16, u'0.180*graph + 0.175*human + 0.142*survey + 0.117*time + 0.094*user + 0.084*interface + 0.065*response + 0.050*system + 0.043*computer + 0.026*eps')\n",
      "2017-03-02 11:18:15,478 : INFO : (17, u'0.350*human + 0.173*computer + 0.089*minors + 0.072*eps + 0.068*time + 0.066*system + 0.054*graph + 0.045*user + 0.035*survey + 0.035*trees')\n",
      "2017-03-02 11:18:15,479 : INFO : (18, u'0.229*computer + 0.184*eps + 0.146*minors + 0.117*trees + 0.093*user + 0.081*graph + 0.048*human + 0.035*survey + 0.019*interface + 0.018*response')\n",
      "2017-03-02 11:18:15,481 : INFO : (19, u'0.233*graph + 0.205*human + 0.161*time + 0.088*minors + 0.079*system + 0.064*eps + 0.055*trees + 0.039*computer + 0.039*response + 0.025*interface')\n"
     ]
    }
   ],
   "source": [
    "model = models.RpModel(corpus_tfidf, num_topics=500)\n",
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "model = models.HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 11:56:42,147 : INFO : loading Dictionary object from /tmp/deerwester.dict\n",
      "2017-03-02 11:56:42,149 : INFO : loaded /tmp/deerwester.dict\n",
      "2017-03-02 11:56:42,151 : INFO : loaded corpus index from /tmp/deerwester.mm.index\n",
      "2017-03-02 11:56:42,155 : INFO : initializing corpus reader from /tmp/deerwester.mm\n",
      "2017-03-02 11:56:42,157 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n",
      "2017-03-02 11:56:42,159 : INFO : using serial LSI version on this node\n",
      "2017-03-02 11:56:42,160 : INFO : updating model with new documents\n",
      "2017-03-02 11:56:42,162 : INFO : preparing a new chunk of documents\n",
      "2017-03-02 11:56:42,164 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-03-02 11:56:42,165 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2017-03-02 11:56:42,167 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2017-03-02 11:56:42,170 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-03-02 11:56:42,172 : INFO : computing the final decomposition\n",
      "2017-03-02 11:56:42,173 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2017-03-02 11:56:42,174 : INFO : processed documents up to #9\n",
      "2017-03-02 11:56:42,176 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"response\" + 0.265*\"time\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2017-03-02 11:56:42,177 : INFO : topic #1(2.542): 0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"response\" + 0.107*\"time\" + -0.072*\"interface\"\n",
      "2017-03-02 11:56:42,179 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-03-02 11:56:42,182 : INFO : creating matrix with 9 documents and 2 features\n",
      "2017-03-02 11:56:42,189 : INFO : saving MatrixSimilarity object under /tmp/deerwester.index, separately None\n",
      "2017-03-02 11:56:42,191 : INFO : saved /tmp/deerwester.index\n",
      "2017-03-02 11:56:42,192 : INFO : loading MatrixSimilarity object from /tmp/deerwester.index\n",
      "2017-03-02 11:56:42,193 : INFO : loaded /tmp/deerwester.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n",
      "[(0, 0.46182100453271646), (1, -0.070027665279000118)]\n",
      "[(0, 0.99809301), (1, 0.93748635), (2, 0.99844527), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.098794639), (8, 0.050041765)]\n",
      "[(2, 0.99844527), (0, 0.99809301), (3, 0.9865886), (1, 0.93748635), (4, 0.90755945), (8, 0.050041765), (7, -0.098794639), (6, -0.10639259), (5, -0.12416792)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "corpus = corpora.MmCorpus('/tmp/deerwester.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)\n",
    "\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it\n",
    "index.save('/tmp/deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples\n",
    "\n",
    "'''\n",
    "[(2, 0.99844527), # The EPS user interface management system\n",
    "(0, 0.99809301), # Human machine interface for lab abc computer applications\n",
    "(3, 0.9865886), # System and human system engineering testing of EPS\n",
    "(1, 0.93748635), # A survey of user opinion of computer system response time\n",
    "(4, 0.90755945), # Relation of user perceived response time to error measurement\n",
    "(8, 0.050041795), # Graph minors A survey\n",
    "(7, -0.098794639), # Graph minors IV Widths of trees and well quasi ordering\n",
    "(6, -0.1063926), # The intersection graph of paths in trees\n",
    "(5, -0.12416792)] # The generation of random binary unordered trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unilever",
   "language": "python",
   "name": "unilever"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
